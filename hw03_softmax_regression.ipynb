{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw03_20171667.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiIduTNGIkqNkCuqz293I0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJRdR6t4SHZZ"
      },
      "source": [
        "import torch\n",
        "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6,], [1,6,6,6], [1,7,7,7] ])\n",
        "y_train = torch.FloatTensor([ [ 0,0,1],  [0,0,1],     [0,0,1],    [0,1,0],     [0,1,0],   [0,1,0],     [1,0,0],    [1,0,0] ])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxI44-r_SycJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21a09bc-0475-4fcf-ac49-d5479b7f81ba"
      },
      "source": [
        "W = torch.zeros(4,3, requires_grad= True)\n",
        "b = torch.zeros(1, 3, requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.Adam([W,b], lr=0.1)\n",
        "\n",
        "for epoch in range(3001):\n",
        "    hypothesis = torch.softmax(torch.mm(x_train, W) + b, dim = 1)\n",
        "    cost = -(y_train * torch.log(hypothesis)).sum(dim=1).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 300 == 0:\n",
        "      print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 1.098612\n",
            "epoch: 300, cost: 0.105262\n",
            "epoch: 600, cost: 0.042634\n",
            "epoch: 900, cost: 0.023111\n",
            "epoch: 1200, cost: 0.014479\n",
            "epoch: 1500, cost: 0.009879\n",
            "epoch: 1800, cost: 0.007124\n",
            "epoch: 2100, cost: 0.005338\n",
            "epoch: 2400, cost: 0.004113\n",
            "epoch: 2700, cost: 0.003236\n",
            "epoch: 3000, cost: 0.002588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hotwflxu112y",
        "outputId": "be8df29c-e9b3-45ba-c127-15d274076f77"
      },
      "source": [
        "W"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-15.2253,   5.8876,   7.3765],\n",
              "        [ -0.6394,   0.4501,   0.7652],\n",
              "        [ 11.6614,  -3.5366,  -9.6394],\n",
              "        [ -3.3187,   2.7671,   1.3850]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOK_RS4BTkZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d52b6a-dbc8-4e4d-9332-cb4a8b47a596"
      },
      "source": [
        "#x에 따른 y값 추측 결과\n",
        "\n",
        "W.requires_grad_(False)\n",
        "b.requires_grad_(False)\n",
        "\n",
        "x_test = torch.FloatTensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]])\n",
        "test_all = torch.softmax(torch.mm(x_test, W)+b, dim=1)\n",
        "print(test_all)\n",
        "print(torch.argmax(test_all, dim=1))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 5.5165e-19, 7.0151e-38],\n",
            "        [1.4800e-02, 7.4294e-01, 2.4226e-01],\n",
            "        [1.2256e-33, 9.0835e-12, 1.0000e+00]])\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7pbDEfhUpli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6c6348-8ce2-4f7b-da51-211dbe7f73a2"
      },
      "source": [
        "# [0,0,1] -> 2, [0,1,0] -> 1, [1,0,0] ->0\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6,], [1,6,6,6], [1,7,7,7] ])\n",
        "y_train = torch.LongTensor([2, 2, 2, 1,1,1,0, 0 ])\n",
        "\n",
        "W = torch.zeros(4, 3, requires_grad= True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.Adam([W,b], lr=1.1)\n",
        "\n",
        "for epoch in range(3001):\n",
        "    #hypothesis = torch.softmax(torch.mm(x_train, W) + b, dim = 1)\n",
        "    #cost = -(y_train * torch.log(hypothesis)).sum(dim=1).mean()\n",
        "    z = torch.mm(x_train, W) + b\n",
        "    cost = F.cross_entropy(z, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 300 == 0:\n",
        "      print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 1.098612\n",
            "epoch: 300, cost: 0.063074\n",
            "epoch: 600, cost: 0.024144\n",
            "epoch: 900, cost: 0.012856\n",
            "epoch: 1200, cost: 0.008008\n",
            "epoch: 1500, cost: 0.005454\n",
            "epoch: 1800, cost: 0.003932\n",
            "epoch: 2100, cost: 0.002947\n",
            "epoch: 2400, cost: 0.002272\n",
            "epoch: 2700, cost: 0.001788\n",
            "epoch: 3000, cost: 0.001431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QHX9Q2EW9CP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63a38c7-9d92-4f9b-86f4-e8fce30ec84f"
      },
      "source": [
        "# forch.nn을 활용한 cross_entropy and softmax\n",
        "import torch.nn as nn\n",
        "\n",
        "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6,], [1,6,6,6], [1,7,7,7] ])\n",
        "y_train = torch.LongTensor([2, 2, 2, 1,1,1,0, 0 ])\n",
        "\n",
        "W = torch.zeros(4, 3, requires_grad= True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "model = nn.Linear(4,3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1.1)\n",
        "\n",
        "for epoch in range(3001):\n",
        "    #hypothesis = torch.softmax(torch.mm(x_train, W) + b, dim = 1)\n",
        "    #cost = -(y_train * torch.log(hypothesis)).sum(dim=1).mean()\n",
        "    #z = torch.mm(x_train, W) + b\n",
        "    z = model(x_train)\n",
        "    cost = F.cross_entropy(z, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "      print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 2.215629\n",
            "epoch: 100, cost: 0.077959\n",
            "epoch: 200, cost: 0.039193\n",
            "epoch: 300, cost: 0.023833\n",
            "epoch: 400, cost: 0.016439\n",
            "epoch: 500, cost: 0.012081\n",
            "epoch: 600, cost: 0.009285\n",
            "epoch: 700, cost: 0.007377\n",
            "epoch: 800, cost: 0.006012\n",
            "epoch: 900, cost: 0.004999\n",
            "epoch: 1000, cost: 0.004225\n",
            "epoch: 1100, cost: 0.003618\n",
            "epoch: 1200, cost: 0.003133\n",
            "epoch: 1300, cost: 0.002739\n",
            "epoch: 1400, cost: 0.002414\n",
            "epoch: 1500, cost: 0.002142\n",
            "epoch: 1600, cost: 0.001912\n",
            "epoch: 1700, cost: 0.001716\n",
            "epoch: 1800, cost: 0.001548\n",
            "epoch: 1900, cost: 0.001402\n",
            "epoch: 2000, cost: 0.001274\n",
            "epoch: 2100, cost: 0.001162\n",
            "epoch: 2200, cost: 0.001063\n",
            "epoch: 2300, cost: 0.000975\n",
            "epoch: 2400, cost: 0.000897\n",
            "epoch: 2500, cost: 0.000827\n",
            "epoch: 2600, cost: 0.000764\n",
            "epoch: 2700, cost: 0.000707\n",
            "epoch: 2800, cost: 0.000655\n",
            "epoch: 2900, cost: 0.000608\n",
            "epoch: 3000, cost: 0.000566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhr7w7NYYFk5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}